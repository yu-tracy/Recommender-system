{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 453,
      "metadata": {
        "id": "f5NDXrN2CtH7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import gzip\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feWoOrmt4Tja",
        "outputId": "37fa34ba-5fc3-4695-cc6f-e46e0e047b96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-03-24 15:50:46--  http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Software_5.json.gz\n",
            "Resolving deepyeti.ucsd.edu... 169.228.63.50\n",
            "Connecting to deepyeti.ucsd.edu|169.228.63.50|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5339013 (5.1M) [application/octet-stream]\n",
            "Saving to: 'Software_5.json.gz.2'\n",
            "\n",
            "Software_5.json.gz. 100%[===================>]   5.09M  1.25MB/s    in 4.7s    \n",
            "\n",
            "2022-03-24 15:50:51 (1.08 MB/s) - 'Software_5.json.gz.2' saved [5339013/5339013]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Software_5.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 458,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7igYuRaV4bF7",
        "outputId": "f82533a4-8cb0-474c-c98e-51b652183aec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obervations in the whole dataset: 12805\n"
          ]
        }
      ],
      "source": [
        "### load the data\n",
        "data = []\n",
        "with gzip.open('Software_5.json.gz') as f:\n",
        "    for l in f:\n",
        "        data.append(json.loads(l.strip()))\n",
        "\n",
        "# convert list into pandas dataframe\n",
        "df = pd.DataFrame.from_dict(data)\n",
        "\n",
        "print(\"Obervations in the whole dataset:\", len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgfA0HWx1i38"
      },
      "source": [
        "Clean the dataset from missing ratings and duplicates (cases where the same user has rated the same item multiple times) if any."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 457,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgWrDtZ94w89",
        "outputId": "08f32818-2137-4162-9498-6bcbf6232984"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obervations in the cleaned dataset: 11884\n"
          ]
        }
      ],
      "source": [
        "df = df.sort_values(by=['reviewerID', 'asin', 'unixReviewTime'])\n",
        "cleaned_dataset = df.dropna(subset=['overall']).drop_duplicates(subset=['reviewerID', 'asin'], keep = 'last').reset_index(drop=True)\n",
        "print(\"Obervations in the cleaned dataset:\", len(cleaned_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8m30AIW1_Rq"
      },
      "source": [
        "create a test set by extracting the latest positively rated item (rating ≥ 4) by each user. Remove users that do not appear in the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 459,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Vq32rQd5D9W",
        "outputId": "45ad0ed0-b336-4fb7-eb0a-d167a060b1c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observations in training set: 10171\n",
            "Observations in test set: 1711\n"
          ]
        }
      ],
      "source": [
        "cleaned_dataset = cleaned_dataset.sort_values(by=['reviewerID', 'unixReviewTime']).reset_index(drop=True)\n",
        "# extracting the latest (in time) positively rated item (rating  ≥4 ) by each user. \n",
        "test_data_pre = cleaned_dataset[cleaned_dataset.overall >= 4.0].drop_duplicates(subset=['reviewerID'], keep='last')\n",
        "# generate training data\n",
        "training_data = cleaned_dataset.drop(test_data_pre.index)\n",
        "print(\"Observations in training set:\", len(training_data))\n",
        "\n",
        "# Remove users that do not appear in the training set.\n",
        "user_in_training = test_data_pre['reviewerID'].isin(training_data['reviewerID'])\n",
        "test_data = test_data_pre[user_in_training]\n",
        "print(\"Observations in test set:\", len(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oXfDxGK2CKf"
      },
      "source": [
        "distribution of ratings per user\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 531,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LBLzsFU5LJ_",
        "outputId": "5069c4ca-a34a-4830-b439-68ea2c93c999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "summary statistics of ratings per user:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "count    1824.000000\n",
              "mean        5.576206\n",
              "std         3.488828\n",
              "min         1.000000\n",
              "25%         4.000000\n",
              "50%         5.000000\n",
              "75%         6.000000\n",
              "max        51.000000\n",
              "Name: overall, dtype: float64"
            ]
          },
          "execution_count": 531,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "number_of_rating_user = training_data.groupby(['reviewerID']).count()\n",
        "print(\"summary statistics of ratings per user:\")\n",
        "number_of_rating_user.overall.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 537,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# plot histogram\n",
        "statistics_user = training_data.reviewerID.value_counts().tolist()\n",
        "x = np.unique(statistics_user)\n",
        "counted = Counter(statistics_user)\n",
        "y = []\n",
        "for key in x:\n",
        "    tmp = counted[key] / len(statistics_user)\n",
        "    y.append(tmp)\n",
        "plt.bar(x, y)\n",
        "plt.xlabel(\"Number of ratings\")\n",
        "plt.ylabel(\"Percentage of users\")\n",
        "plt.savefig(\"distributions_user.pdf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK7VWmiP555Y"
      },
      "source": [
        "distribution of ratings per item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 461,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqZcdGp-5cui",
        "outputId": "f42827e4-dae1-4a61-f37a-2002266abea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "summary statistics of ratings per item:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "count    800.000000\n",
              "mean      12.713750\n",
              "std       16.234801\n",
              "min        1.000000\n",
              "25%        5.000000\n",
              "50%        9.500000\n",
              "75%       17.000000\n",
              "max      226.000000\n",
              "Name: overall, dtype: float64"
            ]
          },
          "execution_count": 461,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "number_of_rating_item = training_data.groupby(['asin']).count()\n",
        "print(\"summary statistics of ratings per item:\")\n",
        "number_of_rating_item.overall.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 538,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# a barplot of the number of ratings ordered by decreasing frequency\n",
        "statistics_item = training_data.asin.value_counts()\n",
        "plt.bar(np.arange(len(statistics_item)), statistics_item)\n",
        "plt.xlabel(\"Item index ordered by decreasing frequency\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.savefig(\"distributions_item.pdf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qsmFKVp6Ge1"
      },
      "source": [
        " the top 5 most popular items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 465,
      "metadata": {
        "id": "0469GyT46Hoh"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rated times</th>\n",
              "      <th>mean rating</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>asin</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>B0001FS9NE</th>\n",
              "      <td>226</td>\n",
              "      <td>4.929204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B000050ZRE</th>\n",
              "      <td>226</td>\n",
              "      <td>4.929204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B0000AZJY6</th>\n",
              "      <td>224</td>\n",
              "      <td>4.928571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B00EZPXYP4</th>\n",
              "      <td>91</td>\n",
              "      <td>4.384615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B00F8K9MZQ</th>\n",
              "      <td>66</td>\n",
              "      <td>4.318182</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            rated times  mean rating\n",
              "asin                                \n",
              "B0001FS9NE          226     4.929204\n",
              "B000050ZRE          226     4.929204\n",
              "B0000AZJY6          224     4.928571\n",
              "B00EZPXYP4           91     4.384615\n",
              "B00F8K9MZQ           66     4.318182"
            ]
          },
          "execution_count": 465,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "popular_rating = number_of_rating_item.overall.sort_values(ascending=False)\n",
        "popular_items = pd.DataFrame(data = popular_rating.values, index = popular_rating.index, columns=[\"rated times\"])\n",
        "user_item_training = training_data.pivot('reviewerID', 'asin', 'overall')\n",
        "user_item_training_mean = user_item_training.mean(axis = 0)\n",
        "for item in popular_items.index.values:\n",
        "    popular_items.loc[item, \"mean rating\"] = user_item_training_mean.loc[item]\n",
        "\n",
        "popular_items.sort_values(by=[\"rated times\", \"mean rating\"], ascending=False).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The metadata file contains information of all items in the complete dataset, not only the 5-core subset. You need to filter out all items that are not included in the training and/or test sets after steps (2) and (3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I did this part in Week 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 466,
      "metadata": {},
      "outputs": [],
      "source": [
        "from surprise import Reader\n",
        "from surprise import Dataset\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 467,
      "metadata": {},
      "outputs": [],
      "source": [
        "reader = Reader(rating_scale=(1, 5))\n",
        "training = Dataset.load_from_df(training_data[['reviewerID', 'asin', 'overall']], reader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define a user-based neighborhood model that takes into account the mean rating of each user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use 3-fold cross-validation on the training set to tune the hyperparameters of the chosen model (similarity measure and number of neighbors for the neighborhood-based model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 468,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n"
          ]
        }
      ],
      "source": [
        "from surprise.model_selection import cross_validate\n",
        "from surprise import KNNWithMeans\n",
        "from surprise.model_selection import GridSearchCV\n",
        "\n",
        "similarities = ['cosine', 'msd', 'pearson', 'pearson_baseline']\n",
        "k_set = [1, 5, 10, 15, 20] # values of k\n",
        "\n",
        "param_grid = {'k': k_set,\n",
        "              'sim_options': {'name': similarities,\n",
        "                              'user_based': [True]\n",
        "                            }}\n",
        "gs_knn = GridSearchCV(KNNWithMeans, param_grid, measures=['rmse'], cv=3)\n",
        "gs_knn.fit(training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Report the optimal hyperparameters together with the corresponding validation Root Mean Square Errors averaged over the 3 folds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 469,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal hyperparameters: {'k': 5, 'sim_options': {'name': 'pearson', 'user_based': True}}\n",
            "Root Mean Square Errors averaged over the 3 folds: 1.213551003412549\n"
          ]
        }
      ],
      "source": [
        "print(\"Optimal hyperparameters:\", gs_knn.best_params['rmse'])\n",
        "print(\"Root Mean Square Errors averaged over the 3 folds:\", gs_knn.best_score['rmse'])\n",
        "# get optimal hyperparameters\n",
        "best_k = gs_knn.best_params['rmse']['k']\n",
        "best_sim_options = gs_knn.best_params['rmse']['sim_options']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the models with the optimal hyperparameters to the whole training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 483,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 483,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainset_nb = training.build_full_trainset()\n",
        "model_nb = KNNWithMeans(k=best_k, sim_options=best_sim_options)\n",
        "model_nb.fit(trainset_nb)\n",
        "testset_nb = trainset_nb.build_anti_testset()\n",
        "predictions_nb = model_nb.test(testset_nb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define an SVD model with user and item biases that uses Stochastic Gradient Descend (SGD) to estimate the low-rank matrix based on only observed ratings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use 3-fold cross-validation on the training set to tune the hyperparameters of the chosen models (number of latent factors and number of epochs for the latent factor model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 471,
      "metadata": {},
      "outputs": [],
      "source": [
        "from surprise import SVD\n",
        "\n",
        "n_factors_set = np.arange(10, 51, 10) \n",
        "n_epochs_set = np.arange(100, 501, 100)\n",
        "\n",
        "param_grid = {'n_epochs': n_epochs_set, 'n_factors': n_factors_set}\n",
        "gs_svd = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
        "gs_svd.fit(training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Report the optimal hyperparameters together with the corresponding validation Root Mean Square Errors averaged over the 3 folds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 484,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal hyperparameters: {'n_epochs': 100, 'n_factors': 10}\n",
            "Root Mean Square Errors averaged over the 3 folds: 1.1344357969390773\n"
          ]
        }
      ],
      "source": [
        "print(\"Optimal hyperparameters:\", gs_svd.best_params['rmse'])\n",
        "print(\"Root Mean Square Errors averaged over the 3 folds:\", gs_svd.best_score['rmse'])\n",
        "\n",
        "best_factors = gs_svd.best_params['rmse']['n_factors']\n",
        "best_epochs = gs_svd.best_params['rmse']['n_epochs']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the models with the optimal hyperparameters to the whole training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 485,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainset = training.build_full_trainset()\n",
        "model_lf = SVD(n_factors = best_factors, n_epochs = best_epochs, random_state = 0)\n",
        "model_lf.fit(trainset)\n",
        "testset_lf = trainset.build_anti_testset()\n",
        "predictions_lf = model_lf.test(testset_lf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the final models to rank the non-rated items for each user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I did this part in Week 8, because we need to remove users of predictions who are in the training set and are not in the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Measure the error of the system’s predicted ratings for Software products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 486,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 113 users in the training set that are not in the test set.\n",
            "Evaluating the systems with 1359246 predictions for users in the test split.\n"
          ]
        }
      ],
      "source": [
        "from surprise import accuracy\n",
        "\n",
        "pred_nb_list = predictions_nb\n",
        "pred_lf_list = predictions_lf\n",
        "# Detect users from training set that are not in test\n",
        "nb_users = set([pred.uid for pred in pred_nb_list])\n",
        "lf_users = set([pred.uid for pred in pred_lf_list])\n",
        "nb_users_in_pred_but_not_in_test = list(nb_users.difference(set(test_data['reviewerID'])))\n",
        "lf_users_in_pred_but_not_in_test = list(lf_users.difference(set(test_data['reviewerID'])))\n",
        "assert nb_users_in_pred_but_not_in_test == lf_users_in_pred_but_not_in_test\n",
        "print(f\"There are {len(lf_users_in_pred_but_not_in_test)} users in the training set that are not in the test set.\")\n",
        "\n",
        "# Remove these users' predictions for evaluation\n",
        "pred_nb_list_removed = [pred for pred in pred_nb_list if pred.uid not in nb_users_in_pred_but_not_in_test]\n",
        "pred_lf_list_removed = [pred for pred in pred_lf_list if pred.uid not in lf_users_in_pred_but_not_in_test]\n",
        "assert len(pred_nb_list_removed) == len(pred_lf_list_removed)\n",
        "print(f\"Evaluating the systems with {len(pred_nb_list_removed)} predictions for users in the test split.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 487,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE: 0.9793\n",
            "RMSE for Neighborhood based Collaborative Filtering: 0.979\n",
            "RMSE: 0.8193\n",
            "RMSE for Latent Factor based Collaborative Filtering: 0.819\n"
          ]
        }
      ],
      "source": [
        "print(\"RMSE for Neighborhood based Collaborative Filtering: {:.3f}\".format(accuracy.rmse(pred_nb_list_removed)))\n",
        "print(\"RMSE for Latent Factor based Collaborative Filtering: {:.3f}\".format(accuracy.rmse(pred_lf_list_removed)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate the top-k (with k = 5) recommendation for each test user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 488,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def transfer_to_user_item_rating(pred_list):\n",
        "    # map users to their massages (iid, est)\n",
        "    user_item_rating = defaultdict(list)\n",
        "    for uid, iid, _, est, _ in pred_list:\n",
        "        user_item_rating[uid].append((iid, est))\n",
        "    # {uid: (iid, est)}\n",
        "    return user_item_rating\n",
        "\n",
        "def top_k_recommendations(n, user_item_rating):\n",
        "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
        "    top_n = defaultdict(list)\n",
        "    for uid, user_ratings in user_item_rating.items():\n",
        "        # rank the non-rated items for each user\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        # {uid: (iid, est)} -> top-n\n",
        "        top_n[uid] = user_ratings[:n]\n",
        "\n",
        "    return top_n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 489,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_item_test = test_data.pivot('reviewerID', 'asin', 'overall')\n",
        "user_item_test = user_item_test.fillna(0)\n",
        "\n",
        "# compute P@k for one user\n",
        "def precision_at_k(k, user_ratings):\n",
        "    n_rel = sum((relevant) for (_, _, relevant) in user_ratings[:k])\n",
        "    return n_rel / k\n",
        "\n",
        "# compute RR@k for one user\n",
        "def RR_at_k(k, user_ratings):\n",
        "    for i in range(k):\n",
        "        _, _, relevant = user_ratings[i]\n",
        "        if relevant == 1:\n",
        "            return 1.0 / (i+1)\n",
        "    return 0.0\n",
        "\n",
        "def total_relevant(user_ratings):\n",
        "    return sum((relevant) for (_, _, relevant) in user_ratings)\n",
        "\n",
        "def transfer_to_user_item_rating_relevant(pred_list):\n",
        "    # map users to their messages\n",
        "    user_item_rating = defaultdict(list)\n",
        "    for pred in pred_list:\n",
        "        true_rating = user_item_test.loc[pred.uid, pred.iid] if pred.iid in list(user_item_test.columns) else 0\n",
        "        relevant = 1 if true_rating >= 4.0 else 0\n",
        "        user_item_rating[pred.uid].append((pred.iid, pred.est, relevant))\n",
        "    # {uid: [(iid, est, relevant)]}\n",
        "    return user_item_rating\n",
        "\n",
        "def compute_metrics(k, user_item_rating):\n",
        "    precisions = dict() # precision\n",
        "    ap = dict() # average precision\n",
        "    rr = dict() # reciprocal rank\n",
        "    for uid, user_ratings in user_item_rating.items():\n",
        "        # rank the non-rated items for each user\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        precisions[uid] = precision_at_k(k, user_ratings)\n",
        "        ap[uid] = (sum((precision_at_k(i, user_ratings)*user_ratings[i-1][2]) for i in range(1, k+1)) / total_relevant(user_ratings)) if total_relevant(user_ratings) != 0 else 0\n",
        "        rr[uid] = RR_at_k(k, user_ratings)\n",
        "\n",
        "    return sum(prec for prec in precisions.values()) / len(precisions), sum(prec for prec in ap.values()) / len(ap), sum(prec for prec in rr.values()) / len(rr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 490,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for Neighborhood based CF:\n",
            "Averaged P@5: 0.004\n",
            "MAP@5: 0.009\n",
            "MRR@5: 0.009\n",
            "\n",
            "Metrics for Latent Factor based CF:\n",
            "Averaged P@5: 0.001\n",
            "MAP@5: 0.002\n",
            "MRR@5: 0.002\n"
          ]
        }
      ],
      "source": [
        "user_item_rating_nb_relevant = transfer_to_user_item_rating_relevant(pred_nb_list_removed)\n",
        "user_item_rating_lf_relevant = transfer_to_user_item_rating_relevant(pred_lf_list_removed)\n",
        "\n",
        "k_set = [5]\n",
        "print(\"Metrics for Neighborhood based CF:\")\n",
        "for k in k_set:\n",
        "    p_at_k_nb, map_at_k_nb, mrr_at_k_nb = compute_metrics(k, user_item_rating_nb_relevant)\n",
        "    print(f\"Averaged P@{k}:\", round(p_at_k_nb, 3))\n",
        "    print(f\"MAP@{k}:\", round(map_at_k_nb, 3))\n",
        "    print(f\"MRR@{k}:\", round(mrr_at_k_nb, 3))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print(\"Metrics for Latent Factor based CF:\")\n",
        "for k in k_set:\n",
        "    p_at_k_lf, map_at_k_lf, mrr_at_k_lf = compute_metrics(k, user_item_rating_lf_relevant)\n",
        "    print(f\"Averaged P@{k}:\", round(p_at_k_lf, 3))\n",
        "    print(f\"MAP@{k}:\", round(map_at_k_lf, 3))\n",
        "    print(f\"MRR@{k}:\", round(mrr_at_k_lf, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the system’s hit rate averaged over the total number of users in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 491,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hit Rate for Neighborhood based CF:\n",
            "Hit Rate (top-5): 0.019\n",
            "\n",
            "Hit Rate for Latent Factor based CF:\n",
            "Hit Rate (top-5): 0.005\n"
          ]
        }
      ],
      "source": [
        "# compute HR@k for one user\n",
        "def HR_at_k(k, user_ratings):\n",
        "    for i in range(k):\n",
        "        _, _, relevant = user_ratings[i]\n",
        "        if relevant == 1:\n",
        "            return 1.0\n",
        "    return 0.0\n",
        "\n",
        "def compute_hit_rate(k, user_item_rating):\n",
        "    hr = dict() # hit rate\n",
        "    for uid, user_ratings in user_item_rating.items():\n",
        "        # Sort user ratings by estimated value\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        hr[uid] = HR_at_k(k, user_ratings)\n",
        "\n",
        "    return sum(prec for prec in hr.values()) / len(hr) \n",
        "\n",
        "k_set = [5]\n",
        "print(\"Hit Rate for Neighborhood based CF:\")\n",
        "for k in k_set:\n",
        "    mhr_at_k_nb = compute_hit_rate(k, user_item_rating_nb_relevant)\n",
        "    print(f\"Hit Rate (top-{k}):\", round(mhr_at_k_nb, 3))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print(\"Hit Rate for Latent Factor based CF:\")\n",
        "for k in k_set:\n",
        "    mhr_at_k_lf = compute_hit_rate(k, user_item_rating_lf_relevant)\n",
        "    print(f\"Hit Rate (top-{k}):\", round(mhr_at_k_lf, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ordered by the value of the column “unixReviewTime”, take the first and last users from the test set as reference and retrieve the 10 nearest neighbours of each reference user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 492,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The first user:  A2G0O4Y8QE10AE\n",
            "The last user:  A2SACTIFMC5DXO\n",
            "10 nearest neighbors of the first user:  ['A100UD67AHFODS', 'A105S56ODHGJEK', 'A1075X1Q4M3S78', 'A10C5CJK1YKGV0', 'A10CRW7XRJBJ2G', 'A10EIJM2C94M14', 'A10G3LHNAK4GEH', 'A10G4BPT5MGBHY', 'A10GU5NVTA5I67', 'A10NC6ZVVMBHNH']\n",
            "10 nearest neighbors of the last user:  ['A23E9QQHJLNGUI', 'A2DV14M0BC6YY1', 'A100UD67AHFODS', 'A105S56ODHGJEK', 'A1075X1Q4M3S78', 'A10C5CJK1YKGV0', 'A10CRW7XRJBJ2G', 'A10EIJM2C94M14', 'A10G3LHNAK4GEH', 'A10G4BPT5MGBHY']\n"
          ]
        }
      ],
      "source": [
        "test_data_unix = test_data.sort_values(by=['unixReviewTime']).reset_index(drop=True)\n",
        "\n",
        "first_user = test_data_unix.loc[0]['reviewerID']\n",
        "last_user = test_data_unix.loc[len(test_data_unix) - 1]['reviewerID']\n",
        "\n",
        "print(\"The first user: \", first_user)\n",
        "print(\"The last user: \", last_user)\n",
        "\n",
        "# get reference users' inner id\n",
        "first_inner_uid = model_nb.trainset.to_inner_uid(first_user)\n",
        "last_inner_uid = model_nb.trainset.to_inner_uid(last_user)\n",
        "\n",
        "# get the 10 neighbors' inner id for the first user\n",
        "first_10_inner_uid = model_nb.get_neighbors(first_inner_uid, k=10)\n",
        "# get the 10 neighbors' inner id for the last user\n",
        "last_10_inner_uid = model_nb.get_neighbors(last_inner_uid, k=10)\n",
        "\n",
        "# get the raw id\n",
        "first_10_raw_uid = [model_nb.trainset.to_raw_uid(inner_id) for inner_id in first_10_inner_uid]\n",
        "last_10_raw_uid = [model_nb.trainset.to_raw_uid(inner_id) for inner_id in last_10_inner_uid]\n",
        "\n",
        "print(\"10 nearest neighbors of the first user: \", first_10_raw_uid)\n",
        "print(\"10 nearest neighbors of the last user: \", last_10_raw_uid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print their rate history and analyse their predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 556,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarities between the first user's ratings and its 10 nearest neighbors' ratings:\n",
            "[1.         0.97491908 1.         0.99362761 0.99513379 1.\n",
            " 0.99931987 0.99199394 0.99975567 0.99701224]\n",
            "\n",
            "Similarities between the last user's ratings and its 10 nearest neighbors' ratings:\n",
            "[0.98379798 0.98604279 0.99720711 0.97853894 0.99720711 0.99249852\n",
            " 0.99361356 0.99720711 0.99670568 0.99128762]\n"
          ]
        }
      ],
      "source": [
        "# get two clusters\n",
        "cluster_first = [first_user] + first_10_raw_uid\n",
        "cluster_last = [last_user] + last_10_raw_uid\n",
        "\n",
        "first_rated_history = training_data[training_data['reviewerID'].isin(cluster_first)]\n",
        "last_rated_history = training_data[training_data['reviewerID'].isin(cluster_last)]\n",
        "\n",
        "user_item_first_cluster = first_rated_history.pivot('reviewerID', 'asin', 'overall')\n",
        "for user in user_item_first_cluster.index.values:\n",
        "    mean_rating_of_user = user_item_first_cluster.loc[user].mean()\n",
        "    # fill in the average rating\n",
        "    user_item_first_cluster.loc[user] = user_item_first_cluster.loc[user].fillna(mean_rating_of_user)\n",
        "\n",
        "user_item_last_cluster = last_rated_history.pivot('reviewerID', 'asin', 'overall')\n",
        "for user in user_item_last_cluster.index.values:\n",
        "    mean_rating_of_user = user_item_last_cluster.loc[user].mean()\n",
        "    # fill in the average rating\n",
        "    user_item_last_cluster.loc[user] = user_item_last_cluster.loc[user].fillna(mean_rating_of_user)\n",
        "\n",
        "print(\"Similarities between the first user's ratings and its 10 nearest neighbors' ratings:\")\n",
        "print(cosine_similarity([user_item_first_cluster.loc[first_user]], user_item_first_cluster.loc[first_10_raw_uid])[0])\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print(\"Similarities between the last user's ratings and its 10 nearest neighbors' ratings:\")\n",
        "print(cosine_similarity([user_item_last_cluster.loc[last_user]], user_item_last_cluster.loc[last_10_raw_uid])[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For those users or products that your model performs poorly on (RR ≤ 0.05), discuss the potential reasons behind."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 521,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of users in the two clusters: 14\n",
            "Total number of users whoes RR <= 0.05: 14\n",
            "Print those users that the model performs poorly on (RR ≤ 0.05):\n",
            "{'A2G0O4Y8QE10AE': 0.001594896331738437, 'A100UD67AHFODS': 0.004608294930875576, 'A105S56ODHGJEK': 0.002898550724637681, 'A1075X1Q4M3S78': 0.02040816326530612, 'A10C5CJK1YKGV0': 0.0022123893805309734, 'A10CRW7XRJBJ2G': 0.0025575447570332483, 'A10EIJM2C94M14': 0.0027100271002710027, 'A10G3LHNAK4GEH': 0.02127659574468085, 'A10G4BPT5MGBHY': 0.0024813895781637717, 'A10GU5NVTA5I67': 0.0038314176245210726, 'A10NC6ZVVMBHNH': 0.0013315579227696406, 'A2SACTIFMC5DXO': 0.009433962264150943, 'A23E9QQHJLNGUI': 0.0014814814814814814, 'A2DV14M0BC6YY1': 0}\n"
          ]
        }
      ],
      "source": [
        "def RR_poorly(user_item_rating):\n",
        "    rr_list = dict() # reciprocal rank\n",
        "    for uid in cluster_first + cluster_last: # set default value as 0\n",
        "        rr_list[uid] = 0\n",
        "    print(\"Total number of users in the two clusters:\", len(rr_list))\n",
        "    for uid, user_ratings in user_item_rating.items():\n",
        "        # rank the non-rated items for each user\n",
        "        if uid in cluster_first or uid in cluster_last:\n",
        "            user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "            user_rr = RR_at_k(len(user_ratings), user_ratings)\n",
        "            if user_rr <= 0.05: # get poor rr\n",
        "                rr_list[uid] = user_rr\n",
        "            # if user_rr != 0.0:\n",
        "            #     print(user_rr)\n",
        "    return rr_list\n",
        "\n",
        "rr_poorly_list = RR_poorly(user_item_rating_nb_relevant)\n",
        "print(\"Total number of users whoes RR <= 0.05:\", len(rr_poorly_list))\n",
        "print(\"Print those users that the model performs poorly on (RR ≤ 0.05):\")\n",
        "print(rr_poorly_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 498,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import gzip\n",
        "import os\n",
        "from urllib.request import urlopen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 500,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-03-29 11:33:34--  http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles2/meta_Software.json.gz\n",
            "Resolving deepyeti.ucsd.edu... 169.228.63.50\n",
            "Connecting to deepyeti.ucsd.edu|169.228.63.50|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15240894 (15M) [application/octet-stream]\n",
            "Saving to: 'meta_Software.json.gz.3'\n",
            "\n",
            "meta_Software.json. 100%[===================>]  14.53M  2.41MB/s    in 7.2s    \n",
            "\n",
            "2022-03-29 11:33:42 (2.02 MB/s) - 'meta_Software.json.gz.3' saved [15240894/15240894]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles2/meta_Software.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 501,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of items in 'meta_Software': 26790\n",
            "Number of items after discard duplicate items: 21639\n",
            "Number of items that were rated by users in training or testing data: 801\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "import pandas as pd\n",
        "\n",
        "# Load the METADATA (ITEMS)\n",
        "def parse(path):\n",
        "    g = gzip.open(path, 'rb')\n",
        "    for l in g:\n",
        "        yield json.loads(l)\n",
        "\n",
        "def getDF(path):\n",
        "  i = 0\n",
        "  df = {}\n",
        "  for d in parse(path):\n",
        "    df[i] = d\n",
        "    i += 1\n",
        "  return pd.DataFrame.from_dict(df, orient='index')\n",
        "\n",
        "df = getDF('meta_Software.json.gz')\n",
        "print(\"Total number of items in \\'meta_Software\\':\", len(df))\n",
        "\n",
        "# Discard duplicates\n",
        "df = df.drop_duplicates(subset=['asin']).reset_index(drop=True)\n",
        "print(\"Number of items after discard duplicate items:\", len(df))\n",
        "\n",
        "# Discard items that weren't rated by our subset of users\n",
        "item_in_training = df['asin'].isin(training_data.append(test_data)['asin'])\n",
        "df = df[item_in_training].reset_index(drop=True)\n",
        "print(\"Number of items that were rated by users in training or testing data:\", len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Select the column “title” from the metadata (only for the products rated in our small subset of the dataset) and apply the following preprocessing to clean up the data: tokenization, transform to lowercase, remove stop- words3, stemming. Report the vocabulary size after preprocessing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 522,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "before preprocessing: 967\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/wy/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /Users/wy/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string \n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "df['clean_title']= df['title'].apply(lambda x: word_tokenize(x))\n",
        "\n",
        "# the vocabulary size before the preprocessing.\n",
        "vacabulary = set()\n",
        "\n",
        "for word in df.clean_title.values:\n",
        "    for voc in word:\n",
        "        vacabulary.add(voc)\n",
        "\n",
        "print(\"before preprocessing:\", len(vacabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 523,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the vocabulary size after preprocessing: 622\n"
          ]
        }
      ],
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "\n",
        "# stopwords removal\n",
        "def remove_stopwords(text):\n",
        "    output= [i for i in text if i not in stop_words]\n",
        "    return output\n",
        "\n",
        "def is_alpha(text):\n",
        "    output = [i for i in text if i.isalpha()]\n",
        "    return output\n",
        "\n",
        "def to_lower(text):\n",
        "    output= [i.lower() for i in text]\n",
        "    return output\n",
        "\n",
        "#defining the object for stemming\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "def stemming(text):\n",
        "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
        "    return stem_text\n",
        "\n",
        "# print(df.clean_title)\n",
        "df['clean_title']= df['clean_title'].apply(lambda x: to_lower(x))\n",
        "df['clean_title']= df['clean_title'].apply(lambda x: is_alpha(x))\n",
        "df['clean_title'] = df['clean_title'].apply(lambda x: remove_stopwords(x))\n",
        "df['clean_title'] = df['clean_title'].apply(lambda x: stemming(x))\n",
        "\n",
        "# the vocabulary size after the preprocessing.\n",
        "vacabulary = set()\n",
        "for word in df.clean_title.values:\n",
        "    for voc in word:\n",
        "        vacabulary.add(voc)\n",
        "print(\"the vocabulary size after preprocessing:\", len(vacabulary))\n",
        "# print(df.clean_title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Represent each product in the TF-IDF vector space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 524,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF matrix shape: (801, 618)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "title_list = []\n",
        "for text in df.clean_title.values:\n",
        "    title_list.append(TreebankWordDetokenizer().detokenize(text))\n",
        "tf_idf_matrix = tfidf_vectorizer.fit_transform(title_list)\n",
        "tf_idf_array = tf_idf_matrix.toarray()\n",
        "\n",
        "print(\"TF-IDF matrix shape:\", tf_idf_array.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Represent each product using pretrained word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gensim.downloader\n",
        "word2vec_vectors = gensim.downloader.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 408,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def convert_sentence2vec(sentence):\n",
        "    vectors = np.zeros((300,))\n",
        "    count = 0\n",
        "    for text in sentence:\n",
        "        try: \n",
        "            vec = word2vec_vectors[text]\n",
        "        except:\n",
        "            pass\n",
        "        else:\n",
        "            count += 1\n",
        "            vectors += vec\n",
        "    result = vectors / count if count != 0 else np.zeros((300,))\n",
        "    return result\n",
        "\n",
        "word_vector = []\n",
        "for text in df.clean_title.values:\n",
        "    vector = convert_sentence2vec(text)\n",
        "    word_vector.append(vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Explore the similarity between products within the vector spaces by computing their cosine similarity. Compare results obtained with TF-IDF and the word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 475,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine similarity of the first five products obtained with TF-IDF:\n",
            "[[1.         0.40876432 0.38100113 0.         0.        ]\n",
            " [0.40876432 1.         0.66379832 0.         0.        ]\n",
            " [0.38100113 0.66379832 1.         0.         0.        ]\n",
            " [0.         0.         0.         1.         0.        ]\n",
            " [0.         0.         0.         0.         1.        ]]\n",
            "\n",
            "Cosine similarity of the first five products obtained with word embeddings:\n",
            "[[1.         0.49886109 0.48628955 0.64383875 0.3779223 ]\n",
            " [0.49886109 1.         0.82148934 0.40978907 0.32713615]\n",
            " [0.48628955 0.82148934 1.         0.43663537 0.3267278 ]\n",
            " [0.64383875 0.40978907 0.43663537 1.         0.41686391]\n",
            " [0.3779223  0.32713615 0.3267278  0.41686391 1.        ]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Cosine similarity between the first five products obtained with TF-IDF:\")\n",
        "print(cosine_similarity(tf_idf_array[:5]))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print(\"Cosine similarity between the first five products obtained with word embeddings:\")\n",
        "print(cosine_similarity(word_vector[:5]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transform the “title” column of each product into a TF-IDF score or other numerical value, e.g., token-count based, that can represent the summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 541,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 5.  8. 10.  7.  4.]\n"
          ]
        }
      ],
      "source": [
        "# get tf-idf score of each item based on token-count \n",
        "def get_tf_idf_score(tf_idf_array):\n",
        "    tf_idf_score = np.zeros(len(tf_idf_array))\n",
        "    for i in range(len(tf_idf_array)):\n",
        "        tf_idf_score[i] = sum(tf_idf_array[i] != 0)\n",
        "    return tf_idf_score\n",
        "\n",
        "tf_idf_score = get_tf_idf_score(tf_idf_array)\n",
        "# print the first 5 items' tf-idf score\n",
        "print(tf_idf_score[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After you represent each product in a vector space, represent each user in the same vector space. This can be done by using an average of the items the user rates. Instead of a simple average, a weighted average can be used, where the weight is the rating for the item by the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 542,
      "metadata": {},
      "outputs": [],
      "source": [
        "item_tf_idf_matrix = pd.DataFrame(index=df.asin.values, data=tf_idf_array)\n",
        "\n",
        "def convert_2_same_vector_space():\n",
        "    # map userID to the corresponding vector space\n",
        "    same_vector_space = defaultdict(pd.DataFrame)\n",
        "    for user in user_item_training.index.values:\n",
        "        # get the rows where 'reviewerID == user'\n",
        "        row_rated_by_users = training_data['reviewerID'].isin([user])\n",
        "        # get items rated by the user\n",
        "        items = training_data[row_rated_by_users].asin.values\n",
        "        # get the user's ratings for these items\n",
        "        ratings = user_item_training.loc[user, items].values\n",
        "        # get items' feature vector from the TF-IDF vector\n",
        "        product_vector = item_tf_idf_matrix.loc[items]\n",
        "        # represent in the same vector space\n",
        "        product_vector['overall'] = ratings\n",
        "        same_vector_space[user] = product_vector\n",
        "    return same_vector_space\n",
        "\n",
        "def build_user_profile(same_vector_space):\n",
        "    # map userID to the user's profile\n",
        "    user_profile = defaultdict()\n",
        "    for user, vector in same_vector_space.items():\n",
        "        # get item feature vectors\n",
        "        item_feature = vector.drop(['overall'], axis=1)\n",
        "        # get the ratings for the items by the user\n",
        "        rating = vector.overall.values\n",
        "        # compute the weighted average\n",
        "        user_profile[user] = np.mean(np.array(item_feature.multiply(rating, axis=0).values), axis=0)\n",
        "    return user_profile\n",
        "\n",
        "same_vector_space = convert_2_same_vector_space()\n",
        "user_profile = build_user_profile(same_vector_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the user-item rating for an item by using a similarity metric between the user and the item. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 543,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_similarity_for_user_profile(user_profile):\n",
        "    # map userID to cosine similarity\n",
        "    similarity = defaultdict()\n",
        "    for user, _ in same_vector_space.items():\n",
        "        # get the rows where 'reviewerID == user'\n",
        "        row_rated_by_users = training_data['reviewerID'].isin([user])\n",
        "        # get items rated by the user\n",
        "        rated_by_user = training_data[row_rated_by_users].asin.values\n",
        "        # get items not rated by the user\n",
        "        not_rated_by = item_tf_idf_matrix.drop(rated_by_user)\n",
        "        temp = pd.DataFrame(index=not_rated_by.index, columns=['cosine_similarity'])\n",
        "        # compute the cosine similarity between user prfile and items not rated by the user\n",
        "        cosine_similarities = cosine_similarity([user_profile[user]], not_rated_by)\n",
        "        temp['cosine_similarity'] = cosine_similarities[0]\n",
        "        similarity[user] = temp\n",
        "    return similarity\n",
        "\n",
        "similarity_for_user_profile = compute_similarity_for_user_profile(user_profile)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Report Precision@5, MAP@5, MRR@5 and the hit rate using the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 544,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transfer_to_user_item_similarity_relevant(similarity):\n",
        "    # map the predictions to each user.\n",
        "    user_item_rating = defaultdict(list)\n",
        "    for user, sim in similarity.items():\n",
        "        if user not in list(user_item_test.index):  # very important part\n",
        "            continue\n",
        "        for iid in sim.index.values:\n",
        "            true_rating = user_item_test.loc[user, iid] if iid in list(user_item_test.columns) else 0\n",
        "            relevant = 1 if true_rating >= 4.0 else 0\n",
        "            user_item_rating[user].append((iid, sim.loc[iid].values[0], relevant))\n",
        "    # {uid: [(iid, similarity, relevant)]}\n",
        "    return user_item_rating\n",
        "\n",
        "# compute HR@k for one user\n",
        "def HR_at_k(k, user_ratings):\n",
        "    for i in range(k):\n",
        "        _, _, relevant = user_ratings[i]\n",
        "        if relevant == 1:\n",
        "            return 1.0\n",
        "    return 0.0\n",
        "\n",
        "def compute_metrics(k, user_item_rating):\n",
        "    hr = dict() # hit rate\n",
        "    precisions = dict() # precision\n",
        "    ap = dict() # average precision\n",
        "    rr = dict() # reciprocal rank\n",
        "    for uid, user_ratings in user_item_rating.items():\n",
        "        # Sort user ratings by estimated value\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        hr[uid] = HR_at_k(k, user_ratings)\n",
        "        precisions[uid] = precision_at_k(k, user_ratings)\n",
        "        ap[uid] = (sum((precision_at_k(i, user_ratings)*user_ratings[i-1][2]) for i in range(1, k+1)) / total_relevant(user_ratings)) if total_relevant(user_ratings) != 0 else 0\n",
        "        rr[uid] = RR_at_k(k, user_ratings)\n",
        "    return sum(prec for prec in hr.values()) / len(hr), sum(prec for prec in precisions.values()) / len(precisions), sum(prec for prec in ap.values()) / len(ap), sum(prec for prec in rr.values()) / len(rr) \n",
        "\n",
        "user_item_similarity_cb = transfer_to_user_item_similarity_relevant(similarity_for_user_profile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 545,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Precision@5: 0.042 \n",
            "MAP@5: 0.162 \n",
            "MRR@5: 0.162 \n",
            "Hit Rate (top-5): 0.208\n"
          ]
        }
      ],
      "source": [
        "mhr_at_k, mp_at_k, map_at_k, mrr_at_k = compute_metrics(5, user_item_similarity_cb)\n",
        "print(f\"Average Precision@5: {round(mp_at_k, 3)} \\nMAP@5: {round(map_at_k, 3)} \\nMRR@5: {round(mrr_at_k, 3)} \\nHit Rate (top-5): {round(mhr_at_k, 3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Weighted strategy** that re-ranks the items by combining the individual rankings from the two models with some aggregate function such as the sum, average, minimum or maximum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 546,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to rank the predictions/similarity of two models\n",
        "def rank_scores(user_item_relevant):\n",
        "    rank = dict()\n",
        "    for uid, msg in user_item_relevant.items():\n",
        "        item_est = pd.DataFrame(data=msg, columns=[\"asin\", \"score\", \"relevant\"])\n",
        "        # Normalize the scores\n",
        "        item_est[\"norm_score\"] = (item_est[\"score\"] - item_est[\"score\"].mean()) / item_est[\"score\"].std() if item_est[\"score\"].std() != 0 else 0\n",
        "        # rank the scores\n",
        "        rank[uid] = item_est.sort_values(by=[\"norm_score\"], ascending=False).set_index(\"asin\", drop=True)\n",
        "    return rank\n",
        "\n",
        "cf_prediction_rank = rank_scores(user_item_rating_nb_relevant)\n",
        "cb_similarity_rank = rank_scores(user_item_similarity_cb)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 547,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rank_combination(cf_prediction_rank, cb_similarity_rank):\n",
        "    rank = defaultdict(list)\n",
        "    for uid, rank_msg_cf in cf_prediction_rank.items():\n",
        "        rank_msg_cb = cb_similarity_rank[uid]\n",
        "        for item in rank_msg_cf.index.values:\n",
        "            norm_in_cf = rank_msg_cf.loc[item, \"norm_score\"]\n",
        "            norm_in_cb = rank_msg_cb.loc[item, \"norm_score\"]\n",
        "            relevant = rank_msg_cb.loc[item, \"relevant\"]\n",
        "            alpha = 1\n",
        "            beta = 1\n",
        "            rank[uid].append((item, alpha*norm_in_cf + beta*norm_in_cb, relevant))\n",
        "\n",
        "    # I did the following sort when computing MRR, MAP such metrics\n",
        "    # for uid, rank_msg in rank.items():\n",
        "    #     rank_msg.sort(key=lambda x: x[1], reverse=True)\n",
        "    #     rank[uid] = rank_msg\n",
        "    return rank\n",
        "\n",
        "rank_combined_weighted = rank_combination(cf_prediction_rank, cb_similarity_rank)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Switching strategy** that uses the recommendations from the collaborative filtering model for some users and the recommendations from the content- based model for other users chosen by a predefined condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 548,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_RR(user_item_rating):\n",
        "    rr_list = dict() # reciprocal rank\n",
        "    for uid, user_ratings in user_item_rating.items():\n",
        "        # rank the non-rated items for each user\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        rr_list[uid] = RR_at_k(len(user_ratings), user_ratings)\n",
        "    return rr_list\n",
        "\n",
        "# compute RR of each user\n",
        "rr_nb = compute_RR(user_item_rating_nb_relevant)\n",
        "rr_cb = compute_RR(user_item_similarity_cb)\n",
        "\n",
        "def check_predefined_condition(userID):\n",
        "    # check RR from two models\n",
        "    return rr_cb[userID] >= rr_nb[userID]\n",
        "\n",
        "def switching_strategy(cf_prediction_rank, cb_similarity_rank):\n",
        "    rank = defaultdict(list)\n",
        "    for uid, rank_msg_cf in cf_prediction_rank.items():\n",
        "        rank_msg_cb = cb_similarity_rank[uid]\n",
        "        if check_predefined_condition(uid):\n",
        "            for item in rank_msg_cb.index.values:\n",
        "                rank[uid].append((item, rank_msg_cb.loc[item, \"norm_score\"], rank_msg_cb.loc[item, \"relevant\"]))\n",
        "        else:\n",
        "            for item in rank_msg_cf.index.values:\n",
        "                rank[uid].append((item, rank_msg_cb.loc[item, \"norm_score\"], rank_msg_cb.loc[item, \"relevant\"]))\n",
        "                \n",
        "    # I did the following sort when computing MRR, MAP such metrics\n",
        "    # for uid, rank_msg in rank.items():\n",
        "    #     rank_msg.sort(key=lambda x: x[1], reverse=True)\n",
        "    #     rank[uid] = rank_msg\n",
        "    return rank\n",
        "\n",
        "rank_combined_switching = switching_strategy(cf_prediction_rank, cb_similarity_rank)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Meta-level strategy** where a level of one model is used as input to the other model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 549,
      "metadata": {},
      "outputs": [],
      "source": [
        "def user_non_rated_item():\n",
        "    non_rated = dict()\n",
        "    user_ids = user_item_training.index.values\n",
        "    item_ids = user_item_training.columns.values\n",
        "    for item in item_ids:\n",
        "        # users rated the item and their ratings\n",
        "        item_rated_by = pd.DataFrame(user_item_training[user_item_training[item] >= 0.0][item])\n",
        "        for user in user_ids:\n",
        "        # check if the user rated the item\n",
        "            if user not in item_rated_by.index.values:\n",
        "                # add to dict, so that we can predict\n",
        "                non_rated[(user, item)] = item_rated_by\n",
        "                \n",
        "    return non_rated\n",
        "\n",
        "non_rated = user_non_rated_item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 550,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_cosine_similarity(non_rated):\n",
        "    # compute the similarities\n",
        "    non_rated_and_similarity = dict()\n",
        "    for (user, item), msg in non_rated.items():\n",
        "        users_rate_item = msg.index.values\n",
        "        user_profiles = list(map(user_profile.get, users_rate_item))\n",
        "        msg[\"similarity\"] = cosine_similarity([user_profile[user]], user_profiles)[0]\n",
        "        non_rated_and_similarity[(user, item)] = msg\n",
        "    return non_rated_and_similarity\n",
        "\n",
        "non_rated_and_similarity = add_cosine_similarity(non_rated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 551,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_predictions(k, non_rated_and_similarity):\n",
        "    # compute predictions\n",
        "    predictions_user = defaultdict(list)\n",
        "    for (user, item), msg in non_rated_and_similarity.items():\n",
        "        three_most_simi = msg.sort_values(by=[\"similarity\"], ascending=False).head(k)\n",
        "        pred = three_most_simi[item].multiply(three_most_simi[\"similarity\"]).sum(axis=0) / three_most_simi[\"similarity\"].sum(axis=0) if three_most_simi[\"similarity\"].sum(axis=0) != 0 else 0\n",
        "        predictions_user[user].append((item, pred))\n",
        "    return predictions_user\n",
        "\n",
        "pred_hybrid = get_predictions(5, non_rated_and_similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 552,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_relevent(pred_hybrid):\n",
        "    # to check if the item is relevant\n",
        "    pred_relevent = defaultdict(list)\n",
        "    for user, pred_list in pred_hybrid.items():\n",
        "        if user not in list(user_item_test.index):  # very important part\n",
        "            continue\n",
        "        for (item, pred) in pred_list:\n",
        "            true_rating = user_item_test.loc[user, item] if item in list(user_item_test.columns) else 0\n",
        "            relevant = 1 if true_rating >= 4.0 else 0\n",
        "            pred_relevent[user].append((item, pred, relevant))\n",
        "    return pred_relevent\n",
        "\n",
        "rank_combined_meta = check_relevent(pred_hybrid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Report Precision@5, MAP@5, MRR@5 and the hit rate using the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 553,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for hybrid recommender system with weighted strategy:\n",
            "Average Precision@5: 0.041 \n",
            "MAP@5: 0.16 \n",
            "MRR@5: 0.16 \n",
            "Hit Rate (top-5): 0.204\n",
            "Metrics for hybrid recommender system with switching strategy:\n",
            "Average Precision@5: 0.041 \n",
            "MAP@5: 0.162 \n",
            "MRR@5: 0.162 \n",
            "Hit Rate (top-5): 0.207\n",
            "Metrics for hybrid recommender system with meta-level strategy:\n",
            "Average Precision@5: 0.026 \n",
            "MAP@5: 0.127 \n",
            "MRR@5: 0.127 \n",
            "Hit Rate (top-5): 0.13\n"
          ]
        }
      ],
      "source": [
        "mhr_at_k, mp_at_k, map_at_k, mrr_at_k = compute_metrics(5, rank_combined_weighted)\n",
        "print(\"Metrics for hybrid recommender system with weighted strategy:\")\n",
        "print(f\"Average Precision@5: {round(mp_at_k, 3)} \\nMAP@5: {round(map_at_k, 3)} \\nMRR@5: {round(mrr_at_k, 3)} \\nHit Rate (top-5): {round(mhr_at_k, 3)}\")\n",
        "\n",
        "mhr_at_k, mp_at_k, map_at_k, mrr_at_k = compute_metrics(5, rank_combined_switching)\n",
        "print(\"Metrics for hybrid recommender system with switching strategy:\")\n",
        "print(f\"Average Precision@5: {round(mp_at_k, 3)} \\nMAP@5: {round(map_at_k, 3)} \\nMRR@5: {round(mrr_at_k, 3)} \\nHit Rate (top-5): {round(mhr_at_k, 3)}\")\n",
        "\n",
        "mhr_at_k, mp_at_k, map_at_k, mrr_at_k = compute_metrics(5, rank_combined_meta)\n",
        "print(\"Metrics for hybrid recommender system with meta-level strategy:\")\n",
        "print(f\"Average Precision@5: {round(mp_at_k, 3)} \\nMAP@5: {round(map_at_k, 3)} \\nMRR@5: {round(mrr_at_k, 3)} \\nHit Rate (top-5): {round(mhr_at_k, 3)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "web_science_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
